import { b, c, d } from './chunk-VO7VPLVP.js';
import { a, b as b$1 } from './chunk-LFFHOQNW.js';
import { useRef, useState, useEffect } from 'react';

var ie={apiKey:"",autoStart:!1,autoTranscribe:!0,mode:"transcriptions",nonStop:!1,removeSilence:!1,stopTimeout:5e3,streaming:!1,timeSlice:1e3,onDataAvailable:void 0,onTranscribe:void 0,customEndpoint:void 0,modelName:void 0},se={stop:void 0},ce={blob:void 0,text:void 0},le=N=>{let{apiKey:m,autoStart:U,autoTranscribe:B,mode:h,nonStop:C,removeSilence:O,stopTimeout:$,streaming:S,timeSlice:j,whisperConfig:u,onDataAvailable:z,onTranscribe:T,customEndpoint:W,modelName:x}={...ie,...N};if(!m&&!T)throw new Error("apiKey is required if onTranscribe is not provided");let f=useRef([]),i=useRef(),c$1=useRef(),t=useRef(),a$1=useRef(),d$1=useRef(se);useRef([]);let [G,k]=useState(!1),[J,E]=useState(!1),[Q,g]=useState(!1),[V,l]=useState(ce);useEffect(()=>()=>{f.current&&(f.current=[]),i.current&&(i.current.flush(),i.current=void 0),t.current&&(t.current.destroy(),t.current=void 0),b$2("stop"),c$1.current&&(c$1.current.off("speaking",R),c$1.current.off("stopped_speaking",v)),a$1.current&&(a$1.current.getTracks().forEach(e=>e.stop()),a$1.current=void 0);},[]),a(async()=>{U&&await F();},[U]);let X=async()=>{await F();},Y=async()=>{await re();},Z=async()=>{await D();},F=async()=>{try{if(a$1.current||await ee(),a$1.current){if(!t.current){let{default:{RecordRTCPromisesHandler:r,StereoAudioRecorder:s}}=await import('recordrtc'),o={mimeType:"audio/wav",numberOfAudioChannels:1,recorderType:s,sampleRate:44100,timeSlice:S?j:void 0,type:"audio",ondataavailable:B&&S?oe:void 0};t.current=new r(a$1.current,o);}if(!i.current){let{Mp3Encoder:r}=await import('lamejs');i.current=new r(1,44100,96);}let e=await t.current.getState();(e==="inactive"||e==="stopped")&&await t.current.startRecording(),e==="paused"&&await t.current.resumeRecording(),C&&H("stop"),k(!0);}}catch{}},ee=async()=>{try{if(a$1.current&&a$1.current.getTracks().forEach(e=>e.stop()),a$1.current=await navigator.mediaDevices.getUserMedia({audio:!0}),!c$1.current){let{default:e}=await import('hark');c$1.current=e(a$1.current,{interval:100,play:!1}),c$1.current.on("speaking",R),c$1.current.on("stopped_speaking",v);}}catch{}},H=e=>{d$1.current[e]||(d$1.current[e]=setTimeout(D,$));},R=()=>{E(!0),b$2("stop");},v=()=>{E(!1),C&&H("stop");},re=async()=>{try{t.current&&(await t.current.getState()==="recording"&&await t.current.pauseRecording(),b$2("stop"),k(!1));}catch{}},D=async()=>{try{if(t.current){let e=await t.current.getState();if((e==="recording"||e==="paused")&&await t.current.stopRecording(),te(),b$2("stop"),k(!1),B)await ne();else {let r=await t.current.getBlob();l({blob:r});}await t.current.destroy(),f.current=[],i.current&&(i.current.flush(),i.current=void 0),t.current=void 0;}}catch{}},te=()=>{c$1.current&&(c$1.current.off("speaking",R),c$1.current.off("stopped_speaking",v),c$1.current=void 0),a$1.current&&(a$1.current.getTracks().forEach(e=>e.stop()),a$1.current=void 0);},b$2=e=>{d$1.current[e]&&(clearTimeout(d$1.current[e]),d$1.current[e]=void 0);},ne=async()=>{try{if(i.current&&t.current&&await t.current.getState()==="stopped"){g(!0);let r=await t.current.getBlob(),s=new Uint8Array(0);if(O){let{createFFmpeg:o}=await import('@ffmpeg/ffmpeg'),n=o({mainName:"main",corePath:b,log:!0});n.isLoaded()||await n.load();let w=await r.arrayBuffer();n.FS("writeFile","in.wav",new Uint8Array(w)),await n.run("-i","in.wav","-acodec","libmp3lame","-b:a","96k","-ar","44100","-af",c,"out.mp3");let A=n.FS("readFile","out.mp3");if(A.length<=225){n.exit(),l({blob:r}),g(!1);return}r=new Blob([A.buffer],{type:"audio/mpeg"}),s=new Uint8Array(A.buffer),n.exit();}else {let o=await r.arrayBuffer(),n=i.current.encodeBuffer(new Int16Array(o));r=new Blob([n],{type:"audio/mpeg"}),s=new Uint8Array(o);}if(typeof T=="function"){let o=await T(r);l(o);}else {let o=new File([r],"speech.mp3",{type:"audio/mpeg"}),n=await _(o);l({blob:r,text:n});}g(!1);}}catch{g(!1);}},oe=async e=>{try{if(S&&t.current){if(z?.(e),i.current){let s=await e.arrayBuffer(),o=i.current.encodeBuffer(new Int16Array(s)),n=new Blob([o],{type:"audio/mpeg"});f.current.push(n);}if(await t.current.getState()==="recording"){let s=new Blob(f.current,{type:"audio/mpeg"}),o=new File([s],"speech.mp3",{type:"audio/mpeg"}),n=await _(o);n&&l(w=>({...w,text:n}));}}}catch{}},_=b$1(async e=>{let r=new FormData;r.append("file",e),x===void 0?r.append("model","whisper-1"):r.append("model",x),h==="transcriptions"&&r.append("language",u?.language??"en"),u?.prompt&&r.append("prompt",u.prompt),u?.response_format&&r.append("response_format",u.response_format),u?.temperature&&r.append("temperature",`${u.temperature}`);let s={};s["Content-Type"]="multipart/form-data",m&&(s.Authorization=`Bearer ${m}`);let{default:o}=await import('axios'),n=d;return W!==void 0&&(n=W),(await o.post(n+h,r,{headers:s})).data.text},[m,h,u]);return {recording:G,speaking:J,transcribing:Q,transcript:V,pauseRecording:Y,startRecording:X,stopRecording:Z}};

export { le as a };
